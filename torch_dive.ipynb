{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SandeeeeeeeeepDey/data-science-11-weeks-progg/blob/main/torch_dive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6wD9Iq4_Hy-x"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOdy3ROOHy-0"
      },
      "source": [
        "\n",
        "**Learn the Basics** ||\n",
        "[Quickstart](quickstart_tutorial.html) ||\n",
        "[Tensors](tensorqs_tutorial.html) ||\n",
        "[Datasets & DataLoaders](data_tutorial.html) ||\n",
        "[Transforms](transforms_tutorial.html) ||\n",
        "[Build Model](buildmodel_tutorial.html) ||\n",
        "[Autograd](autogradqs_tutorial.html) ||\n",
        "[Optimization](optimization_tutorial.html) ||\n",
        "[Save & Load Model](saveloadrun_tutorial.html)\n",
        "\n",
        "# Learn the Basics\n",
        "\n",
        "Authors:\n",
        "[Suraj Subramanian](https://github.com/suraj813),\n",
        "[Seth Juarez](https://github.com/sethjuarez/),\n",
        "[Cassie Breviu](https://github.com/cassieview/),\n",
        "[Dmitry Soshnikov](https://soshnikov.com/),\n",
        "[Ari Bornstein](https://github.com/aribornstein/)\n",
        "\n",
        "Most machine learning workflows involve working with data, creating models, optimizing model\n",
        "parameters, and saving the trained models. This tutorial introduces you to a complete ML workflow\n",
        "implemented in PyTorch, with links to learn more about each of these concepts.\n",
        "\n",
        "We'll use the FashionMNIST dataset to train a neural network that predicts if an input image belongs\n",
        "to one of the following classes: T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker,\n",
        "Bag, or Ankle boot.\n",
        "\n",
        "`This tutorial assumes a basic familiarity with Python and Deep Learning concepts.`\n",
        "\n",
        "\n",
        "## Running the Tutorial Code\n",
        "You can run this tutorial in a couple of ways:\n",
        "\n",
        "- **In the cloud**: This is the easiest way to get started! Each section has a \"Run in Microsoft Learn\" and \"Run in Google Colab\" link at the top, which opens an integrated notebook in Microsoft Learn or Google Colab, respectively, with the code in a fully-hosted environment.\n",
        "- **Locally**: This option requires you to setup PyTorch and TorchVision first on your local machine ([installation instructions](https://pytorch.org/get-started/locally/)). Download the notebook or copy the code into your favorite IDE.\n",
        "\n",
        "\n",
        "## How to Use this Guide\n",
        "If you're familiar with other deep learning frameworks, check out the [0. Quickstart](quickstart_tutorial.html) first\n",
        "to quickly familiarize yourself with PyTorch's API.\n",
        "\n",
        "If you're new to deep learning frameworks, head right into the first section of our step-by-step guide: [1. Tensors](tensor_tutorial.html).\n",
        "\n",
        "\n",
        ".. include:: /beginner_source/basics/qs_toc.txt\n",
        "\n",
        ".. toctree::\n",
        "   :hidden:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "KX51SSyNaU09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor"
      ],
      "metadata": {
        "id": "RLsuOsWez67T"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Datasets"
      ],
      "metadata": {
        "id": "jGnP1lM4aX8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = datasets.FashionMNIST(\n",
        "    root = \"data\",\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = ToTensor()\n",
        ")"
      ],
      "metadata": {
        "id": "bs_mE7T90Wo1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = datasets.FashionMNIST(\n",
        "    root = \"data\",\n",
        "    train = True,\n",
        "    download= True,\n",
        "    transform = ToTensor()\n",
        ")"
      ],
      "metadata": {
        "id": "kP_fMaAj1SqG"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wrap datasets in Dataloader"
      ],
      "metadata": {
        "id": "t3rfBhHhaduz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wrap datasets with dataloader that acts as an iterable over our dataset, and supports automatic batching, sampling, shuffling and multiprocess data loading."
      ],
      "metadata": {
        "id": "7GBMpYsAim5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "train_dataloader = DataLoader(train_data, batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size)"
      ],
      "metadata": {
        "id": "m4T97buS14QU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check dataloader"
      ],
      "metadata": {
        "id": "4xfZvW9PaM-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for X,y in test_dataloader:\n",
        "#   print(X.shape)\n",
        "#   print(y.shape, y.dtype)\n",
        "\n",
        "#torch.Size([64, 1, 28, 28]) 64 pics, with one channel(grascale)(rgb has 3), leagth and width is 28,28\n",
        "#torch.Size([64]) torch.int64  64 labels"
      ],
      "metadata": {
        "id": "cvbrFXaNP11s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Device"
      ],
      "metadata": {
        "id": "_SDYFrJtamfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = (\n",
        "    'cuda'\n",
        "    if torch.cuda.is_available()\n",
        "    else 'mps'\n",
        "    if torch.backends.mps.is_available()\n",
        "    else 'cpu'\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "id": "6-x7TkawQGBR",
        "outputId": "f59fb9be-795c-4d42-ea18-97c9576c8015",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network Class or Model\n"
      ],
      "metadata": {
        "id": "_rBhreyRawjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To define a neural network in PyTorch, we create a class that inherits from nn.Module. We define the layers of the network in the __init__ function and specify how data will pass through the network in the forward function. To accelerate operations in the neural network, we move it to the GPU or MPS if available."
      ],
      "metadata": {
        "id": "kTIWBxuwjuM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.linear_relu_stack = nn.Sequential(\n",
        "        nn.Linear(28*28, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512,512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 10)\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    x = self.flatten(x)\n",
        "    logits = self.linear_relu_stack(x)\n",
        "    return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "pijMrKJG2uQe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5fa355f-8d33-46df-d0a5-684085207b67"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loss fn and Optimizer"
      ],
      "metadata": {
        "id": "Em15YmdPa3Kc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make loss function and optimizer\n",
        "\n",
        "\n",
        "**Optimization** is the process of adjusting model parameters to reduce model error in each training step. Optimization algorithms define how this process is performed (in this example we use Stochastic Gradient Descent). All optimization logic is encapsulated in the optimizer object. Here, we use the SGD optimizer; additionally, there are many different optimizers available in PyTorch such as ADAM and RMSProp, that work better for different kinds of models and data."
      ],
      "metadata": {
        "id": "QAWgYPtAm7lu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn =  nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-25)"
      ],
      "metadata": {
        "id": "3w_VNoRvWSVo"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and Test Fn"
      ],
      "metadata": {
        "id": "EsXSZDC7bMLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  model.train()\n",
        "  for batch, (X,y) in enumerate(dataloader):\n",
        "    X,y = X.to(device), y.to(device)\n",
        "\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred,y)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      loss, current = loss.item(), (batch+1)*len(X)\n",
        "      print(f\"loss = {loss:>7f}, current{current:>5d}, size:{size:>5d}\")"
      ],
      "metadata": {
        "id": "3hQgnd9feT9K"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batch = len(dataloader)\n",
        "\n",
        "  model.eval()\n",
        "  test_loss, correct = 0,0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X,y = X.to(device), y.to(device)\n",
        "\n",
        "      pred = model(X)\n",
        "\n",
        "      test_loss += loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1)==y).type(torch.float).sum().item()\n",
        "\n",
        "  test_loss /= num_batch\n",
        "  correct /= size\n",
        "  print(f\"Acc={(100*correct):>0.1f}, avg loss={test_loss:>8f}\")\n"
      ],
      "metadata": {
        "id": "dlQrXqxTj10r"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Run"
      ],
      "metadata": {
        "id": "IGTl2khvbPrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 5\n",
        "for t in range(epoch):\n",
        "  print(t+1)\n",
        "  train(train_dataloader,model,loss_fn,optimizer)\n",
        "  test(test_dataloader, model, loss_fn)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5CG7XRTpB3O",
        "outputId": "64def2b0-3afd-43b4-9865-439114e78e40"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "loss = 2.297520, current   64, size:60000\n",
            "loss = 2.302744, current 6464, size:60000\n",
            "loss = 2.302143, current12864, size:60000\n",
            "loss = 2.300788, current19264, size:60000\n",
            "loss = 2.301084, current25664, size:60000\n",
            "loss = 2.298843, current32064, size:60000\n",
            "loss = 2.300575, current38464, size:60000\n",
            "loss = 2.301897, current44864, size:60000\n",
            "loss = 2.292238, current51264, size:60000\n",
            "loss = 2.296605, current57664, size:60000\n",
            "Acc=9.5, avg loss=2.301013\n",
            "2\n",
            "loss = 2.297520, current   64, size:60000\n",
            "loss = 2.302744, current 6464, size:60000\n",
            "loss = 2.302143, current12864, size:60000\n",
            "loss = 2.300788, current19264, size:60000\n",
            "loss = 2.301084, current25664, size:60000\n",
            "loss = 2.298843, current32064, size:60000\n",
            "loss = 2.300575, current38464, size:60000\n",
            "loss = 2.301897, current44864, size:60000\n",
            "loss = 2.292238, current51264, size:60000\n",
            "loss = 2.296605, current57664, size:60000\n",
            "Acc=9.5, avg loss=2.301013\n",
            "3\n",
            "loss = 2.297520, current   64, size:60000\n",
            "loss = 2.302744, current 6464, size:60000\n",
            "loss = 2.302143, current12864, size:60000\n",
            "loss = 2.300788, current19264, size:60000\n",
            "loss = 2.301084, current25664, size:60000\n",
            "loss = 2.298843, current32064, size:60000\n",
            "loss = 2.300575, current38464, size:60000\n",
            "loss = 2.301897, current44864, size:60000\n",
            "loss = 2.292238, current51264, size:60000\n",
            "loss = 2.296605, current57664, size:60000\n",
            "Acc=9.5, avg loss=2.301013\n",
            "4\n",
            "loss = 2.297520, current   64, size:60000\n",
            "loss = 2.302744, current 6464, size:60000\n",
            "loss = 2.302143, current12864, size:60000\n",
            "loss = 2.300788, current19264, size:60000\n",
            "loss = 2.301084, current25664, size:60000\n",
            "loss = 2.298843, current32064, size:60000\n",
            "loss = 2.300575, current38464, size:60000\n",
            "loss = 2.301897, current44864, size:60000\n",
            "loss = 2.292238, current51264, size:60000\n",
            "loss = 2.296605, current57664, size:60000\n",
            "Acc=9.5, avg loss=2.301013\n",
            "5\n",
            "loss = 2.297520, current   64, size:60000\n",
            "loss = 2.302744, current 6464, size:60000\n",
            "loss = 2.302143, current12864, size:60000\n",
            "loss = 2.300788, current19264, size:60000\n",
            "loss = 2.301084, current25664, size:60000\n",
            "loss = 2.298843, current32064, size:60000\n",
            "loss = 2.300575, current38464, size:60000\n",
            "loss = 2.301897, current44864, size:60000\n",
            "loss = 2.292238, current51264, size:60000\n",
            "loss = 2.296605, current57664, size:60000\n",
            "Acc=9.5, avg loss=2.301013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Save"
      ],
      "metadata": {
        "id": "OiesNmfExjd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"model.pth\")"
      ],
      "metadata": {
        "id": "tFW_ynmDwWRT"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test saved model"
      ],
      "metadata": {
        "id": "oCdRbaBIbfXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "model.load_state_dict(torch.load(\"model.pth\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-wR0coHx2B2",
        "outputId": "8a0e645e-cc2c-4eae-b0e0-a9681c95aa18"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\n",
        "    \"T-shirt/top\",\n",
        "    \"Trouser\",\n",
        "    \"Pullover\",\n",
        "    \"Dress\",\n",
        "    \"Coat\",\n",
        "    \"Sandal\",\n",
        "    \"Shirt\",\n",
        "    \"Sneaker\",\n",
        "    \"Bag\",\n",
        "    \"Ankle boot\",\n",
        "]\n",
        "\n",
        "model.eval()\n",
        "x, y = test_data[1][0], test_data[1][1]\n",
        "with torch.no_grad():\n",
        "    x = x.to(device)\n",
        "    pred = model(x)\n",
        "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOpU9Jj8yiX7",
        "outputId": "a431d509-84ac-4a2b-84d3-a692158dc533"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: \"Coat\", Actual: \"T-shirt/top\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "grJp6iO8d_cM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}