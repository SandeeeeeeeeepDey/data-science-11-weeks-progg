{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1zux4PdMK/HAullSMxxvf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SandeeeeeeeeepDey/data-science-11-weeks-progg/blob/main/keras_tuner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperperameter Tuning"
      ],
      "metadata": {
        "id": "vZmDRe-rDdKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q -U keras-tuner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g76jsy0kk0Dr",
        "outputId": "be50a157-7870-471f-95af-2499b8ca5937"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/129.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_6bYNT5rkSZY"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras_tuner as kt\n",
        "from time import strftime\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = tf.keras.datasets.fashion_mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Msi3QWjvoyp9",
        "outputId": "74e00827-1466-4a1d-fc08-3d97a42c6163"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(X_full, y_full), (X_test, y_test) = all_data"
      ],
      "metadata": {
        "id": "279vc5glpT_E"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_valid = X_full[:5000], X_full[5000:]\n",
        "y_train, y_valid = y_full[:5000], y_full[5000:]"
      ],
      "metadata": {
        "id": "A8Q0QfeypT4e"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model_builder Function"
      ],
      "metadata": {
        "id": "3kvw158KDiht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_builder(hp):\n",
        "  n_hidden = hp.Int(\"n_hidden\", min_value = 0, max_value = 8, default = 8)\n",
        "  n_neurons = hp.Int(\"n_neuron\", min_value = 16, max_value = 256)\n",
        "  learning_rate = hp.Float(\"learning_rate\", min_value = 1e-4, max_value = 1e-2, sampling = \"log\")\n",
        "  optimizer = hp.Choice(\"optimizer\", values = [\"sgd\", \"adam\"])\n",
        "  if optimizer == \"sgd\":\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate = learning_rate)\n",
        "\n",
        "  if optimizer == \"adam\":\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "  for _ in range(n_hidden):\n",
        "    model.add(tf.keras.layers.Dense(n_neurons, activation = \"relu\"))\n",
        "  model.add(tf.keras.layers.Dense(10, activation = \"softmax\"))\n",
        "  model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = optimizer, metrics = [\"accuracy\"])\n",
        "  return model"
      ],
      "metadata": {
        "id": "seUfdJWLkppC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Search Optimizer"
      ],
      "metadata": {
        "id": "iHAcq1XHD6eu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Randomly searches the different combinations in each trials"
      ],
      "metadata": {
        "id": "P7jiOgSMGmIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining"
      ],
      "metadata": {
        "id": "c_PjG165OzIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_search_tuner = kt.RandomSearch(model_builder, objective = \"val_accuracy\", max_trials = 10, overwrite = True,\n",
        "                                      directory = \"my_fashion_mnist\", project_name = \"my_rand_search\", seed = 42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqV9C8-gntPB",
        "outputId": "506fc1a0-66de-4666-efe1-599fc1b932c5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 10 Complete [00h 03m 10s]\n",
            "val_accuracy: 0.8211636543273926\n",
            "\n",
            "Best val_accuracy So Far: 0.8373636603355408\n",
            "Total elapsed time: 00h 29m 30s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create callbacks"
      ],
      "metadata": {
        "id": "mF6DIFWPO5Nw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\"cp1\", save_weights_only=False)\n",
        "early_bird = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", patience = 10)"
      ],
      "metadata": {
        "id": "Eaix3I1HOt7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tuning"
      ],
      "metadata": {
        "id": "W3DamMYoO8Ge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_search_tuner.search(X_train, y_train, epochs = 30, validation_data = (X_valid, y_valid), callbacks = [checkpoint, early_bird])"
      ],
      "metadata": {
        "id": "RjN4W0HfOxCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####To make use of the models and configs found using tuning"
      ],
      "metadata": {
        "id": "MtQyuawnGv3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top3_models = random_search_tuner.get_best_models(num_models = 3)\n",
        "best_model = top3_models[0]"
      ],
      "metadata": {
        "id": "EDs4CG0EqGKV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top3_params = random_search_tuner.get_best_hyperparameters(num_trials = 3)\n",
        "top3_params[0].values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkCsLJ6cbrIg",
        "outputId": "6c891020-089b-4a24-e921-7ca3873f84ae"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'n_hidden': 7,\n",
              " 'n_neuron': 124,\n",
              " 'learning_rate': 0.0005509513888645584,\n",
              " 'optimizer': 'adam'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each tuner is guided by a so-called oracle: before each trial, the tuner asks\n",
        "the oracle to tell it what the next trial should be.\n",
        "\n",
        "Since the oracle keeps track of all the\n",
        "trials, we can ask it to give you the best one"
      ],
      "metadata": {
        "id": "ToI4xV7Ad1Lc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Doesn't work anymore"
      ],
      "metadata": {
        "id": "yKqf3rjcEoSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_trial = random_search_tuner.get_best_trials(num_trials = 3)[0]\n",
        "best_trial.summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "60fY9NgddyfX",
        "outputId": "ea7884f3-3855-40b7-88df-c399518f779a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'RandomSearch' object has no attribute 'get_best_trials'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-6ca8bd1013ab>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_search_tuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_trials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbest_trial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'RandomSearch' object has no attribute 'get_best_trials'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "can also view the hyperparameter from this"
      ],
      "metadata": {
        "id": "alwzY5XGeZUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_trial.metrics.get_last_value(\"val_accuracy\")"
      ],
      "metadata": {
        "id": "9jdpfbH_eOn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Continuation"
      ],
      "metadata": {
        "id": "18qBP6b3Ev90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model.fit(X_full, y_full, epochs = 200)\n",
        "test_loss, test_accuracy = best_model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "s6gsH-_iemGU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b3f625e-11b0-44ed-80e1-05b9b7279303"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.4599 - accuracy: 0.8371\n",
            "Epoch 2/200\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.4038 - accuracy: 0.8537\n",
            "Epoch 3/200\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.3744 - accuracy: 0.8646\n",
            "Epoch 4/200\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.3551 - accuracy: 0.8718\n",
            "Epoch 5/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.3347 - accuracy: 0.8785\n",
            "Epoch 6/200\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.3214 - accuracy: 0.8837\n",
            "Epoch 7/200\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.3088 - accuracy: 0.8866\n",
            "Epoch 8/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.2985 - accuracy: 0.8905\n",
            "Epoch 9/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.2889 - accuracy: 0.8943\n",
            "Epoch 10/200\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.2833 - accuracy: 0.8956\n",
            "Epoch 11/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.2726 - accuracy: 0.8994\n",
            "Epoch 12/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.2689 - accuracy: 0.9007\n",
            "Epoch 13/200\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.2625 - accuracy: 0.9025\n",
            "Epoch 14/200\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.2578 - accuracy: 0.9052\n",
            "Epoch 15/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.2500 - accuracy: 0.9083\n",
            "Epoch 16/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.2466 - accuracy: 0.9089\n",
            "Epoch 17/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.2414 - accuracy: 0.9109\n",
            "Epoch 18/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.2406 - accuracy: 0.9105\n",
            "Epoch 19/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.2340 - accuracy: 0.9133\n",
            "Epoch 20/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.2298 - accuracy: 0.9147\n",
            "Epoch 21/200\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.2250 - accuracy: 0.9159\n",
            "Epoch 22/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.2237 - accuracy: 0.9165\n",
            "Epoch 23/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.2209 - accuracy: 0.9192\n",
            "Epoch 24/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.2179 - accuracy: 0.9192\n",
            "Epoch 25/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.2159 - accuracy: 0.9200\n",
            "Epoch 26/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.2360 - accuracy: 0.9133\n",
            "Epoch 27/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.2191 - accuracy: 0.9191\n",
            "Epoch 28/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1987 - accuracy: 0.9250\n",
            "Epoch 29/200\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.2073 - accuracy: 0.9237\n",
            "Epoch 30/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1985 - accuracy: 0.9260\n",
            "Epoch 31/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.2102 - accuracy: 0.9258\n",
            "Epoch 32/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1940 - accuracy: 0.9270\n",
            "Epoch 33/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1897 - accuracy: 0.9292\n",
            "Epoch 34/200\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1888 - accuracy: 0.9299\n",
            "Epoch 35/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1862 - accuracy: 0.9308\n",
            "Epoch 36/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1897 - accuracy: 0.9304\n",
            "Epoch 37/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1829 - accuracy: 0.9331\n",
            "Epoch 38/200\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1788 - accuracy: 0.9334\n",
            "Epoch 39/200\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1875 - accuracy: 0.9313\n",
            "Epoch 40/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1755 - accuracy: 0.9342\n",
            "Epoch 41/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1856 - accuracy: 0.9318\n",
            "Epoch 42/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1717 - accuracy: 0.9362\n",
            "Epoch 43/200\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1716 - accuracy: 0.9359\n",
            "Epoch 44/200\n",
            "1875/1875 [==============================] - 10s 6ms/step - loss: 0.1755 - accuracy: 0.9351\n",
            "Epoch 45/200\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1659 - accuracy: 0.9378\n",
            "Epoch 46/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1682 - accuracy: 0.9377\n",
            "Epoch 47/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1799 - accuracy: 0.9331\n",
            "Epoch 48/200\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1639 - accuracy: 0.9387\n",
            "Epoch 49/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1615 - accuracy: 0.9397\n",
            "Epoch 50/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1577 - accuracy: 0.9417\n",
            "Epoch 51/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1635 - accuracy: 0.9392\n",
            "Epoch 52/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1585 - accuracy: 0.9411\n",
            "Epoch 53/200\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1610 - accuracy: 0.9408\n",
            "Epoch 54/200\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1589 - accuracy: 0.9414\n",
            "Epoch 55/200\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1589 - accuracy: 0.9420\n",
            "Epoch 56/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1510 - accuracy: 0.9442\n",
            "Epoch 57/200\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1574 - accuracy: 0.9433\n",
            "Epoch 58/200\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1444 - accuracy: 0.9466\n",
            "Epoch 59/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1472 - accuracy: 0.9459\n",
            "Epoch 60/200\n",
            "1875/1875 [==============================] - 14s 8ms/step - loss: 0.1545 - accuracy: 0.9439\n",
            "Epoch 61/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1441 - accuracy: 0.9480\n",
            "Epoch 62/200\n",
            "1875/1875 [==============================] - 14s 8ms/step - loss: 0.1477 - accuracy: 0.9465\n",
            "Epoch 63/200\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1769 - accuracy: 0.9366\n",
            "Epoch 64/200\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1517 - accuracy: 0.9441\n",
            "Epoch 65/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1389 - accuracy: 0.9492\n",
            "Epoch 66/200\n",
            "1875/1875 [==============================] - 16s 9ms/step - loss: 0.1344 - accuracy: 0.9502\n",
            "Epoch 67/200\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1530 - accuracy: 0.9444\n",
            "Epoch 68/200\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1400 - accuracy: 0.9497\n",
            "Epoch 69/200\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.1409 - accuracy: 0.9496\n",
            "Epoch 70/200\n",
            "1875/1875 [==============================] - 14s 8ms/step - loss: 0.1382 - accuracy: 0.9496\n",
            "Epoch 71/200\n",
            "1875/1875 [==============================] - 16s 8ms/step - loss: 0.1363 - accuracy: 0.9497\n",
            "Epoch 72/200\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1343 - accuracy: 0.9510\n",
            "Epoch 73/200\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1462 - accuracy: 0.9472\n",
            "Epoch 74/200\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.1677 - accuracy: 0.9463\n",
            "Epoch 75/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1713 - accuracy: 0.9386\n",
            "Epoch 76/200\n",
            "1875/1875 [==============================] - 14s 8ms/step - loss: 0.1549 - accuracy: 0.9466\n",
            "Epoch 77/200\n",
            "1875/1875 [==============================] - 17s 9ms/step - loss: 0.1434 - accuracy: 0.9473\n",
            "Epoch 78/200\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1339 - accuracy: 0.9526\n",
            "Epoch 79/200\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1294 - accuracy: 0.9535\n",
            "Epoch 80/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1335 - accuracy: 0.9523\n",
            "Epoch 81/200\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.1224 - accuracy: 0.9555\n",
            "Epoch 82/200\n",
            "1875/1875 [==============================] - 16s 9ms/step - loss: 0.1249 - accuracy: 0.9547\n",
            "Epoch 83/200\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1301 - accuracy: 0.9540\n",
            "Epoch 84/200\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.1586 - accuracy: 0.9451\n",
            "Epoch 85/200\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1212 - accuracy: 0.9559\n",
            "Epoch 86/200\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1348 - accuracy: 0.9531\n",
            "Epoch 87/200\n",
            "1875/1875 [==============================] - 14s 8ms/step - loss: 0.1080 - accuracy: 0.9594\n",
            "Epoch 88/200\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1265 - accuracy: 0.9555\n",
            "Epoch 89/200\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.1201 - accuracy: 0.9565\n",
            "Epoch 90/200\n",
            "1875/1875 [==============================] - 14s 8ms/step - loss: 0.1163 - accuracy: 0.9577\n",
            "Epoch 91/200\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1130 - accuracy: 0.9583\n",
            "Epoch 92/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1269 - accuracy: 0.9565\n",
            "Epoch 93/200\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1135 - accuracy: 0.9579\n",
            "Epoch 94/200\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1300 - accuracy: 0.9540\n",
            "Epoch 95/200\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1297 - accuracy: 0.9545\n",
            "Epoch 96/200\n",
            "1875/1875 [==============================] - 18s 9ms/step - loss: 0.1236 - accuracy: 0.9577\n",
            "Epoch 97/200\n",
            "1875/1875 [==============================] - 16s 8ms/step - loss: 0.1126 - accuracy: 0.9585\n",
            "Epoch 98/200\n",
            "1875/1875 [==============================] - 14s 8ms/step - loss: 0.1163 - accuracy: 0.9585\n",
            "Epoch 99/200\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1079 - accuracy: 0.9613\n",
            "Epoch 100/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1145 - accuracy: 0.9599\n",
            "Epoch 101/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1322 - accuracy: 0.9562\n",
            "Epoch 102/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0993 - accuracy: 0.9633\n",
            "Epoch 103/200\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1040 - accuracy: 0.9630\n",
            "Epoch 104/200\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1155 - accuracy: 0.9605\n",
            "Epoch 105/200\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1014 - accuracy: 0.9639\n",
            "Epoch 106/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1053 - accuracy: 0.9620\n",
            "Epoch 107/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1077 - accuracy: 0.9617\n",
            "Epoch 108/200\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1346 - accuracy: 0.9557\n",
            "Epoch 109/200\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.1036 - accuracy: 0.9636\n",
            "Epoch 110/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1260 - accuracy: 0.9590\n",
            "Epoch 111/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1020 - accuracy: 0.9644\n",
            "Epoch 112/200\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0991 - accuracy: 0.9646\n",
            "Epoch 113/200\n",
            "1875/1875 [==============================] - 16s 8ms/step - loss: 0.0926 - accuracy: 0.9666\n",
            "Epoch 114/200\n",
            "1875/1875 [==============================] - 16s 9ms/step - loss: 0.1052 - accuracy: 0.9634\n",
            "Epoch 115/200\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1034 - accuracy: 0.9643\n",
            "Epoch 116/200\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1029 - accuracy: 0.9652\n",
            "Epoch 117/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1020 - accuracy: 0.9638\n",
            "Epoch 118/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0905 - accuracy: 0.9678\n",
            "Epoch 119/200\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1308 - accuracy: 0.9604\n",
            "Epoch 120/200\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1173 - accuracy: 0.9588\n",
            "Epoch 121/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1031 - accuracy: 0.9623\n",
            "Epoch 122/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1410 - accuracy: 0.9549\n",
            "Epoch 123/200\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1181 - accuracy: 0.9596\n",
            "Epoch 124/200\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.1080 - accuracy: 0.9628\n",
            "Epoch 125/200\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1910 - accuracy: 0.9452\n",
            "Epoch 126/200\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1007 - accuracy: 0.9654\n",
            "Epoch 127/200\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.1000 - accuracy: 0.9647\n",
            "Epoch 128/200\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0963 - accuracy: 0.9664\n",
            "Epoch 129/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0833 - accuracy: 0.9701\n",
            "Epoch 130/200\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1043 - accuracy: 0.9639\n",
            "Epoch 131/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.4563 - accuracy: 0.9653\n",
            "Epoch 132/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1455 - accuracy: 0.9521\n",
            "Epoch 133/200\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1082 - accuracy: 0.9626\n",
            "Epoch 134/200\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1013 - accuracy: 0.9669\n",
            "Epoch 135/200\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0813 - accuracy: 0.9701\n",
            "Epoch 136/200\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0990 - accuracy: 0.9678\n",
            "Epoch 137/200\n",
            "1875/1875 [==============================] - 17s 9ms/step - loss: 0.2773 - accuracy: 0.9429\n",
            "Epoch 138/200\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1730 - accuracy: 0.9468\n",
            "Epoch 139/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0884 - accuracy: 0.9679\n",
            "Epoch 140/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0820 - accuracy: 0.9711\n",
            "Epoch 141/200\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0811 - accuracy: 0.9708\n",
            "Epoch 142/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1003 - accuracy: 0.9651\n",
            "Epoch 143/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0795 - accuracy: 0.9721\n",
            "Epoch 144/200\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1060 - accuracy: 0.9645\n",
            "Epoch 145/200\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1373 - accuracy: 0.9552\n",
            "Epoch 146/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0892 - accuracy: 0.9681\n",
            "Epoch 147/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0801 - accuracy: 0.9720\n",
            "Epoch 148/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0895 - accuracy: 0.9707\n",
            "Epoch 149/200\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0792 - accuracy: 0.9719\n",
            "Epoch 150/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0913 - accuracy: 0.9693\n",
            "Epoch 151/200\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0816 - accuracy: 0.9722\n",
            "Epoch 152/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0983 - accuracy: 0.9674\n",
            "Epoch 153/200\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1021 - accuracy: 0.9675\n",
            "Epoch 154/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1279 - accuracy: 0.9581\n",
            "Epoch 155/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1511 - accuracy: 0.9682\n",
            "Epoch 156/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0705 - accuracy: 0.9751\n",
            "Epoch 157/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1946 - accuracy: 0.9673\n",
            "Epoch 158/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0996 - accuracy: 0.9657\n",
            "Epoch 159/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0899 - accuracy: 0.9715\n",
            "Epoch 160/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0791 - accuracy: 0.9753\n",
            "Epoch 161/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0798 - accuracy: 0.9745\n",
            "Epoch 162/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1047 - accuracy: 0.9662\n",
            "Epoch 163/200\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0678 - accuracy: 0.9754\n",
            "Epoch 164/200\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0736 - accuracy: 0.9740\n",
            "Epoch 165/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1871 - accuracy: 0.9661\n",
            "Epoch 166/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1348 - accuracy: 0.9594\n",
            "Epoch 167/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1100 - accuracy: 0.9633\n",
            "Epoch 168/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0874 - accuracy: 0.9731\n",
            "Epoch 169/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0808 - accuracy: 0.9723\n",
            "Epoch 170/200\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0745 - accuracy: 0.9744\n",
            "Epoch 171/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1398 - accuracy: 0.9537\n",
            "Epoch 172/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0672 - accuracy: 0.9759\n",
            "Epoch 173/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1604 - accuracy: 0.9514\n",
            "Epoch 174/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1135 - accuracy: 0.9646\n",
            "Epoch 175/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0737 - accuracy: 0.9757\n",
            "Epoch 176/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0640 - accuracy: 0.9774\n",
            "Epoch 177/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0924 - accuracy: 0.9707\n",
            "Epoch 178/200\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0783 - accuracy: 0.9746\n",
            "Epoch 179/200\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1107 - accuracy: 0.9692\n",
            "Epoch 180/200\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0681 - accuracy: 0.9774\n",
            "Epoch 181/200\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0815 - accuracy: 0.9742\n",
            "Epoch 182/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0740 - accuracy: 0.9754\n",
            "Epoch 183/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0647 - accuracy: 0.9777\n",
            "Epoch 184/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1279 - accuracy: 0.9617\n",
            "Epoch 185/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0766 - accuracy: 0.9748\n",
            "Epoch 186/200\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.4252 - accuracy: 0.9342\n",
            "Epoch 187/200\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.2628 - accuracy: 0.9248\n",
            "Epoch 188/200\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.1771 - accuracy: 0.9714\n",
            "Epoch 189/200\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1591 - accuracy: 0.9578\n",
            "Epoch 190/200\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1065 - accuracy: 0.9716\n",
            "Epoch 191/200\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0683 - accuracy: 0.9796\n",
            "Epoch 192/200\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1048 - accuracy: 0.9724\n",
            "Epoch 193/200\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1542 - accuracy: 0.9588\n",
            "Epoch 194/200\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.4390 - accuracy: 0.9261\n",
            "Epoch 195/200\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.5308 - accuracy: 0.8904\n",
            "Epoch 196/200\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0965 - accuracy: 0.9730\n",
            "Epoch 197/200\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0815 - accuracy: 0.9755\n",
            "Epoch 198/200\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1438 - accuracy: 0.9649\n",
            "Epoch 199/200\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1423 - accuracy: 0.9617\n",
            "Epoch 200/200\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0898 - accuracy: 0.9748\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.9054 - accuracy: 0.8825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparam_tuning using submodule and the Hyperparameter Tuner function\n",
        "if we want to fine-tune data preprocessing\n",
        "hyperparameters, or model.fit() arguments, such as the batch size"
      ],
      "metadata": {
        "id": "lF7ap7zciwIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyHyperModule(kt.HyperModel):\n",
        "  def build(self,hp):\n",
        "    return model_builder(hp)\n",
        "\n",
        "  def fit(self, hp, model, X, y, **kwargs):\n",
        "    if hp.Boolean(\"normalize\"):\n",
        "      normer = tf.keras.layers.Normalization()\n",
        "      X = normer(X)\n",
        "    return model.fit(X,y, **kwargs)"
      ],
      "metadata": {
        "id": "HMs0gSkU0fjQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperband Optimizer"
      ],
      "metadata": {
        "id": "xeUeF25GDz8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1 (Initialization)**: samples different combinations of hyperparameter configs.\n",
        "\n",
        "**Step 2 (Exploration)**: Tries the configs on a franction of max epochs and discards the worst\n",
        "\n",
        "**Step 3 (Successive Halving)**: Trains the good ones on  more epocs and halves the number of combinations each epoch. Until factor number of top is remaining"
      ],
      "metadata": {
        "id": "13ezuoQ4LnR9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Faster than Random Search but make sure confics can perform better in the long run rather than short term.(eg. have requirement of lower learnitg rate)"
      ],
      "metadata": {
        "id": "8TRLY00ZNEDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hyperband_tuner = kt.Hyperband(MyHyperModule(), objective = \"val_accuracy\", seed = 42, max_epochs = 30, factor = 3,\n",
        "                               hyperband_iterations = 2, overwrite = True, directory = \"fashion_mnist\", project_name = \"hyperband\")"
      ],
      "metadata": {
        "id": "udUJIUGwfnoq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q -U tensorboard_plugin_profile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed1MRSV04H_5",
        "outputId": "e0ab2017-3aba-477a-935f-5d4007981ecc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tensor Board Notes"
      ],
      "metadata": {
        "id": "c87xi_K9EE2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "def get_run_logdir(root_logdir=\"my_logs\"):\n",
        "  return Path(root_logdir) / strftime(\"run_%Y_%m_%d_%H_%M_%S\")\n",
        "  \n",
        "run_logdir = get_run_logdir() # e.g., my_logs/run_2022_08_01_17_25_59\n",
        "```\n",
        "\n",
        "\n",
        "The good news is that Keras provides a convenient TensorBoard()\n",
        "callback that will take care of creating the log directory for you (along with\n",
        "its parent directories if needed), and it will create event files and write\n",
        "summaries to them during training. It will measure your model’s training\n",
        "and validation loss and metrics (in this case, the MSE and RMSE), and it\n",
        "will also profile your neural network. It is straightforward to use:\n",
        " ```\n",
        "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir,\n",
        "profile_batch=(100, 200))\n",
        "```"
      ],
      "metadata": {
        "id": "F5FRbvg34ykh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Callback setups"
      ],
      "metadata": {
        "id": "q1D8dpkHGXG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root_logdir = Path(hyperband_tuner.project_dir) / \"tensorboard\"\n",
        "tensorboard_cb = tf.keras.callbacks.TensorBoard(root_logdir)\n",
        "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience = 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Jx_b1Z43o4j",
        "outputId": "04c1b571-4847-4eb7-8103-8d6538b2cb6f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 180 Complete [00h 01m 00s]\n",
            "val_accuracy: 0.787745475769043\n",
            "\n",
            "Best val_accuracy So Far: 0.8316181898117065\n",
            "Total elapsed time: 01h 21m 24s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tuning"
      ],
      "metadata": {
        "id": "SF490e_rGRwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hyperband_tuner.search(X_train, y_train, epochs = 30, validation_data = (X_valid, y_valid),\n",
        "                       callbacks = [early_stopping_cb, tensorboard_cb])"
      ],
      "metadata": {
        "id": "bExYK7x0GL3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BayesianOptimization implementing Gaussian process"
      ],
      "metadata": {
        "id": "iXSqkic9Cgrr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **alpha**\n",
        "\n",
        "> represents the level of noise you expect\n",
        "in the performance measures across trials (it defaults to 10 )\n",
        "\n",
        "> **beta**\n",
        "\n",
        "> specifies how much you want the algorithm to explore, instead of simply\n",
        "exploiting the known good regions of hyperparameter space"
      ],
      "metadata": {
        "id": "94FBHRlVCs56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bayesian_opt_tuner = kt.BayesianOptimization(\n",
        "    MyHyperModule(), objective = \"val_accuracy\", seed = 42,\n",
        "    max_trial = 10, alpha = 1e-4, beta = 2.6, overwrite = True, directory= \"my_fashion_mnist\", project_name = \"bayesian_opt\"\n",
        ")\n",
        "\n",
        "bayesian_opt_tuner.search(X_train, y_train, epochs = 200, validation_data = (X_valid, y_valid), callbacks = [early_stopping_cb, tensorboard_cb])"
      ],
      "metadata": {
        "id": "4To5gazbCf18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Extra Notes\n"
      ],
      "metadata": {
        "id": "GGbff599FlD-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google\n",
        "has also used an evolutionary approach, not just to search for\n",
        "hyperparameters\n",
        "\n",
        "**but**\n",
        "\n",
        " also to explore all sorts of model architectures:\n",
        "\n",
        " it\n",
        "powers their AutoML service on Google Vertex AI.\n",
        "\n",
        "The\n",
        "term AutoML refers to any system that takes care of a large part of the ML\n",
        "workflow.\n",
        "\n",
        "Evolutionary algorithms have even been used successfully to\n",
        "train individual neural networks, replacing the ubiquitous gradient descent!"
      ],
      "metadata": {
        "id": "So5o_y18Fodr"
      }
    }
  ]
}